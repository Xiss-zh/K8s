



# K8s-V1.23.xx-Kubeadm安装

### 1、基础环境配置

```shell
# 配置主机名
hostnamectl set-hostname k8s-master01

# 配置域名解析
cat >> /etc/hosts << EOF
192.168.10.241 k8s-master01
192.168.10.242 k8s-master02
192.168.10.243 k8s-master03
192.168.10.244 k8s-node01
192.168.10.245 k8s-node02
EOF

# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld

# 关闭selinux
setenforce 0 && sed -ri 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
 
# 关闭swap交换空间
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab

#关闭NetworkManager
systemctl disable --now NetworkManager


#加载所需内核模块
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF > /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1

vm.overcommit_memory=1
net.ipv4.conf.all.route_localnet = 1 
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720
 
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

```

### 2、安装并加载ipvs作为流量转发

```shell
#1，在所有节点安装ipvs
yum install ipset ipvsadm -y

ipvsadm -l -n

#2，在所有节点运行
cat <<EOF> /etc/sysconfig/modules/ipvs.modules
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack


```

### 3、安装Docker

```shell
 #安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2。
 yum install -y yum-utils  device-mapper-persistent-data lvm2
 
 # 选择国内的一些源地址：
 yum-config-manager --add-repo  http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
 
 # 安装最新版本的 Docker Engine-Community 和 containerd
 yum install docker-ce docker-ce-cli containerd.io -y
 
# 启动
systemctl enable --now docker
systemctl enable --now containerd
 
# 配置阿里镜像源，日志大小，cgroup
mkdir -p /etc/docker

cat > /etc/docker/daemon.json << EOF
{
  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

# 加载配置文件重启
systemctl daemon-reload

# 重启
systemctl restart docker

# 查看docker 配置信息
docker info
```

### 4、安装kubelet、kubeadm、kubectl

```shell
#配置阿里源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

#安装kubelet、kubeadm、kubectl
yum install -y --nogpgcheck kubelet-1.23.10 kubeadm-1.23.10 kubectl-1.23.10

systemctl enable --now kubelet

```

### 5、下载k8s镜像

```shell
tee ./images.sh <<'EOF'
#!/bin/bash

images=(
kube-apiserver:v1.23.10
kube-controller-manager:v1.23.10
kube-scheduler:v1.23.10
kube-proxy:v1.23.10
pause:3.6
etcd:3.5.1-0
coredns:v1.8.6
)
for imageName in ${images[@]} ; do
docker pull registry.aliyuncs.com/google_containers/$imageName
done
EOF


chmod +x ./images.sh && ./images.sh
```



### 6、初始化主节点

```shell
#主节点初始化、所有网络范围不重叠

kubeadm init \
--apiserver-advertise-address=192.168.10.241 \
--control-plane-endpoint=k8s-master01 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.23.10 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.172.0.0/16

```



### 7、根据提示操作

![image-20221020152814391](C:\Users\zhangxing\AppData\Roaming\Typora\typora-user-images\image-20221020152814391.png)



### 8、安装网络组件：flannel

```shell
wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

#修改ip地址
vim kube-flannel.yml
"Network": "10.172.0.0/16",

kubectl apply -f kube-flannel.yml
```

### 9、部署dashboard

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml

#设置访问端口
kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard

type: ClusterIP 改为 type: NodePort

#找到端口
kubectl get svc -A |grep kubernetes-dashboard
```

### 10、创建访问账号

```yaml
 vim dash.yaml
 
 apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
  
  
#获取令牌token
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
```



### 11、修改为ipvs模式

```shell
#修改configmap，将mode: " "修改为mode: "ipvs"，:wq保存退出
kubectl edit -n kube-system cm kube-proxy

#查看kube-system命名空间下的kube-proxy并删除，删除后，k8s会自动再次生成，新生成的kube-proxy会采用刚刚配置的ipvs模式
kubectl get pod -n kube-system
kubectl get pod -n kube-system |grep kube-proxy |awk '{system("kubectl delete pod "$1" -n kube-system")}'

    #查看日志，确认使用的是ipvs，或者ipvsadm -Ln
kubectl get pod -n kube-system | grep kube-proxy
kubectl logs -n kube-system kube-proxy-XXX

```



### 12、删除节点

```shell
#在master节上，执行：
kubectl drain 节点名 --delete-local-data --force --ignore-daemonsets

#从集群移出
kubectl delete node 节点名


#在node3机器上，执行：
kubeadm reset

#清除iptable规则
iptables -X
iptables -F
#或者运行
ipvsadm --clear

#删除cni相关配置
rm -rf /etc/cni/
```



### 13、重新添加master节点

```shell
mkdir -p /etc/kubernetes/pki/etcd
scp root@k8s-master01:/etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/
scp root@k8s-master01:/etc/kubernetes/pki/ca.key /etc/kubernetes/pki/
scp root@k8s-master01:/etc/kubernetes/pki/sa.key /etc/kubernetes/pki/
scp root@k8s-master01:/etc/kubernetes/pki/sa.pub /etc/kubernetes/pki/
scp root@k8s-master01:/etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/
scp root@k8s-master01:/etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/
scp root@k8s-master01:/etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/
scp root@k8s-master01:/etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/etcd/
scp root@k8s-master01:/etc/kubernetes/admin.conf /etc/kubernetes/admin.conf

#新令牌
kubeadm token create --print-join-command

kubeadm join k8s-master01:6443 --token 2jye4v.78x944oi00oel0z5 --discovery-token-ca-cert-hash sha256:01fba33fffab3e527e5a2b10f505f33489761b4d0c131bd84cf6f1fd9b0e8e23 --control-plane
```

14、查看添加节点的命令

```
kubeadm token create --print-join-command
```

